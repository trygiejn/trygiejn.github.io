---
title: "Statistical Methods | Applications on Analyzing Wine Quality"
author: "John Trygier"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```

# Suppose the population mean of the variable “density” is µ , do the following inferences:

## a. Provide an estimate of µ based on the sample;

### Set Working Directory

```{r cars}
setwd("C:/Users/John Trygier/Documents/Schoolwork/Graduate/Fall 2021/Statistical Methods/Project/Part A/Data")
rw <- read.csv("winequality-red.csv")
summary(rw)
```

### Transform string data into useful dataframe

```{r}
my_list <- apply(rw, 1, strsplit, split = ";")
# unlist and turn it into a matrix
my_matrix <- matrix(as.numeric(unlist(my_list)), nrow = nrow(rw), 
                    ncol =length(my_list[[1]][[1]]), byrow = TRUE)

rw.df <- as.data.frame(my_matrix)
colnames(rw.df) <- c("Fixed Acidity", "Volatile Acidity", "Citric Acid", "Residual Sugar", "Chlorides", "Free Sulfur Dioxide", "Total Sulfur Dioxide", "Density", "pH", "Sulphates", "Alcohol Volume", "Quality")

```

### Perform calculations

```{r}
mu.hat<-mean(rw.df$Density)
mu.hat
sd.mu.hat<-sd(rw.df$Density)/sqrt(length(rw.df$Density))
sd.mu.hat 
hist(rw.df$Density)
```

> The above graphic visualizes the distribution of the "Density" variable. We can view the distribution of the data through the above histogram and the corresponding approximation of density across the dataset for the variable.

```{r}
density(rw.df$Density)
plot(density(rw.df$Density))
```

> Above we see the distribution and visualization for the density of the "Density" variable. 

## b. Use the Central Limit Theorem (CLT) to quantify the variability of your estimate;

```{r}
sd(rw.df$Density)/sqrt(length(rw.df))
```

> We use the above formula to calculate the standard error for "mu hat" for the "Density" variable. This is used to establish the confidence interval for the dataset.  

## c. Use the CLT to give a 95% confidence interval for µ.

```{r}

mean(rw.df$Density) - 2*sd(rw.df$Density)/sqrt(length(rw.df))
mean(rw.df$Density) + 2*sd(rw.df$Density)/sqrt(length(rw.df))
```

> The 95% confidence interval for the "Density" variable is (0.9957, 0.9978).

## d. Use the bootstrap method to do parts b and c, and compare the results with those obtained from the CLT. State your findings.

```{r}
mu.hat.set<-NULL    ### Used to store mu.hat’s
for(k in 1:2000){   ### Repeat sampling 2000 times
sample<-rpois(n = 1599, lambda = mean(rw.df$Density))
mu.hat<-mean(sample)
mu.hat.set[k]<-mu.hat
}

mu.hat.set<-NULL    ### Used to store mu.hat’s
for(k in 1:2000){   ### Repeat sampling 2000 times
sample.bootstrap<-sample(sample, size=1599, 
replace=T)
mu.hat<-mean(sample.bootstrap)
mu.hat.set[k]<-mu.hat
}
sd(mu.hat.set)
hist(mu.hat.set,freq = FALSE)
lines(density(mu.hat.set), lwd=5, col='blue')

c(mu.hat-2*sd.mu.hat, mu.hat+2*sd.mu.hat)

```

> Here we use the bootstrap method to create a sample using information from our original sample population. The confidence interval generated by the bootstrap method is (0.988, 0.988)

```{r}
sd(mu.hat.set)
mean(mu.hat.set)
```

> These are the metrics we can glean from our sample population, our sample standard deviation is 0.035, with a mean of 0.997. 

```{r}
hist(mu.hat.set,freq = FALSE)
lines(density(mu.hat.set), lwd=5, col='blue')
```

> We can see that Density follows a normal distribution. 

## e. Can we use a normal distribution to model “density”? If yes, what are the maximum likelihood estimates of the mean and standard deviation? Please provide their standard errors as well. 

> Yes We can. 

```{r}
library(stats4)
likelihood.fun<-function(mu){
dnorm(x=mean(rw.df$Density),mean=mu, sd=1)
}
minuslog.lik<-function(mu){   ###-log(likelihood)
-log(likelihood.fun(mu))
}
est <- mle(minuslog=minuslog.lik, start=list(mu=0))
summary(est)
```

```{r}
minuslog.lik<-function(mu, sigma){
log.lik<-0
for(i in 1:1599){
log.lik<-log.lik+log(dnorm(rw.df$Density[i]*10, mean=mu, sd=sigma))
}
return(-log.lik)
}
```

```{r}
est1 <- mle(minuslog=minuslog.lik, 
start=list(mu=mean(rw.df$`Residual Sugar`), sigma=sd(rw.df$`Residual Sugar`)))
summary(est1)
```

# 2. Suppose the population mean of the variable “residual sugar” is µ , answer the following questions.

## a. Provide an estimate of µ based on the sample;

```{r}
mean(rw.df$`Residual Sugar`)
mu <- mean(rw.df$`Residual Sugar`)
```

## b. Noting that the sample distribution of “residual sugar” is highly skewed, can we use the CLT to quantify the variability of your estimate? Can we use the CLT to give a 95% confidence interval for µ? If yes, please give your solution. If no, explain why.

> Yes. CLT is used to measure the distribution of the mean of data across samples, which ultimately becomes normally distributed for any set of data, no mater what the skew is of the original data. 

## c. Use the bootstrap method to do part b. Is the bootstrap confidence interval symmetric? (hint: check the bootstrap distribution; see p. 43 in Lecture 3).

```{r}
mu.hat<-mean(rw.df$`Residual Sugar`)
mu.hat
sd.mu.hat<-sd(rw.df$`Residual Sugar`)/sqrt(length(rw.df$ `Residual Sugar`))
sd.mu.hat 
hist(rw.df$`Residual Sugar`)

mu.hat.set<-NULL    ### Used to store mu.hat’s
for(k in 1:2000){   ### Repeat sampling 2000 times
sample<-rpois(n = 1599, lambda = mean(rw.df$`Residual Sugar`))
mu.hat<-mean(sample)
mu.hat.set[k]<-mu.hat
}

mu.hat.set<-NULL    ### Used to store mu.hat’s
for(k in 1:2000){   ### Repeat sampling 2000 times
sample.bootstrap<-sample(sample, size=1599, 
replace=T)
mu.hat<-mean(sample.bootstrap)
mu.hat.set[k]<-mu.hat
}
sd(mu.hat.set)
hist(mu.hat.set,freq = FALSE)
lines(density(mu.hat.set), lwd=5, col='blue')

c(mu.hat-2*sd.mu.hat, mu.hat+2*sd.mu.hat)
```

```{r}
sd(mu.hat.set)
mean(mu.hat.set)
```

```{r}
hist(mu.hat.set,freq = FALSE)
lines(density(mu.hat.set), lwd=5, col='blue')
```

> The Bootstrap confidence interval is symmetric, as demonstrated by the histogram & density lines in the graphic above. 

## d. Can we use a normal distribution to model “residual sugar”? If no, what distribution do you think can approximate its empirical distribution? What parameters are needed to characterize such a distribution? what are their maximum likelihood estimates? Please provide their standard errors as well. 

```{r}
hist(rw.df$`Residual Sugar`)
```

> We cannot use a normal distribution to model "residual sugar". We can use a lognormal distribution to approximate the empirical distribution of the data. 

```{r}
minuslog.lik<-function(mu, sigma){
    log.lik<-0
    for(i in 1:1599){
        log.lik = log.lik + log(dlnorm(x=rw.df$`Residual Sugar`[i], meanlog = mu, sdlog = sigma))
    }
    return(-log.lik)
}
est.lognorm<-mle(minuslog=minuslog.lik, start=list(mu=log(mean(rw.df$`Residual Sugar`)), 
                                                   sigma=log(sd(rw.df$`Residual Sugar`))))
summary(est.lognorm)
```

> We need the mean, standard deviation, and standard error of the data to characterize that distribution. 


# 3. We classify those wines as “excellent” if their rating is at least 7. Suppose the population proportion of excellent wines is p. Do the following:

> We'll start by creating a new column in the dataset that indicates whether the wine is "excellent" or not. 

```{r}
rw.df$excellent <- ifelse(rw.df$Quality >= 7, 1, 0)
p = sum(rw.df$excellent)/nrow(rw.df)
sum(rw.df$excellent)/nrow(rw.df)
```

> We can see that approximately 13.6% of wines in the dataset can be classified as "excellent". 

## a. Use the CLT to derive a 95% confidence interval for p;

```{r}
mu.hat<-mean(rw.df$excellent)
mu.hat
sd.mu.hat<-sd(rw.df$excellent)/sqrt(length(rw.df$excellent))
sd.mu.hat 
```

> Now that we've established  the standard error and the mean of the data, we can establish our 95% confidence interval.

```{r}
mean(rw.df$excellent) + 2 * sd(rw.df$excellent)/sqrt(length(rw.df$excellent))
mean(rw.df$excellent) - 2 * sd(rw.df$excellent)/sqrt(length(rw.df$excellent))
```

> Our 95% confidence interval is (0.118, 0.153)

## b. Use the bootstrap method to derive a 95% confidence interval for p;

```{r}
mu.hat.set<-NULL    ### Used to store mu.hat’s
for(k in 1:2000){   ### Repeat sampling 2000 times
sample<-rpois(n = 1599, lambda = mean(rw.df$excellent))
mu.hat<-mean(sample)
mu.hat.set[k]<-mu.hat
}

mu.hat.set<-NULL    ### Used to store mu.hat’s
for(k in 1:2000){   ### Repeat sampling 2000 times
sample.bootstrap<-sample(sample, size=1599, 
replace=T)
mu.hat<-mean(sample.bootstrap)
mu.hat.set[k]<-mu.hat
}
sd(mu.hat.set)
hist(mu.hat.set,freq = FALSE)
lines(density(mu.hat.set), lwd=5, col='blue')

c(mu.hat-2*sd.mu.hat, mu.hat+2*sd.mu.hat)
```

## c. Compare the two intervals. Is there any difference worth our attention?

> Above we see the distribution of our estimate mean. The bootstrap method yields a 95% confidence interval of (0.12, 0.16) vs. our CLT estimation of (0.128, 0.153). These intervals are very similar to one another. 


## d. What is the maximum likelihood estimate of p and its standard error? 

```{r}
library(stats4)
likelihood.fun<-function(mu){
dnorm(x=mean(rw.df$excellent),mean=mu, sd=1)
}
minuslog.lik<-function(mu){   ###-log(likelihood)
-log(likelihood.fun(mu))
}
est <- mle(minuslog=minuslog.lik, start=list(mu=0))
summary(est)
```

```{r}
minuslog.lik<-function(mu, sigma){
log.lik<-0
for(i in 1:1599){
log.lik<-log.lik+log(dnorm(rw.df$excellent[i]*10, mean=mu, sd=sigma))
}
return(-log.lik)
}
```

```{r}
est1 <- mle(minuslog=minuslog.lik, 
start=list(mu=mean(rw.df$excellent), sigma=sd(rw.df$excellent)))
summary(est1)
```


